---
title: "CGS698C, Lectures 5-7: Model building in the Bayesian framework"
runningheader: "Bayesian models & data analysis" # only for pdf output
subtitle: "Bayesian models & data analysis" # only for html output
author: "Himanshu Yadav"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout: default
  tufte::tufte_html:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: fombiblio.bib
link-citations: yes
header-includes: |
  \usepackage{tikz}
  \usepackage{enumerate}
  \usepackage[shortlabels]{enumitem}
  \usepackage{amsmath}
  \usepackage{comment}
  \geometry{
  left=25mm, % left margin
  textwidth=160mm, % main text block
  marginparsep=0mm, % gutter between main text block and margin notes
  marginparwidth=25mm % width of margin notes
  }
  \setcounter{secnumdepth}{3}
---

```{r include=FALSE}
knitr::opts_chunk$set(fig.width=6.5, fig.height=3)
library(ggplot2)
library(reshape2)
library(dplyr)
library(stats)
library(brms)
library(bayesplot)
library(invgamma)
library(TruncatedNormal)
library(truncnorm)

graph_book <- function(n_grid){
  p <- ggplot(data.frame(x=1:n_grid,y=1:n_grid),aes(x,y))+
    geom_blank()+theme_bw()+scale_x_continuous(breaks=1:n_grid)+
    scale_y_continuous(breaks = 1:n_grid)+
    theme(axis.text = element_blank())+xlab("")+ylab("")+
    theme(title = element_text(size=10),
          panel.border = element_blank(),
          axis.ticks = element_blank())
  return(p)
}
```


\newcommand{\mybox}[1]{%
         \begin{center}%
            \begin{tikzpicture}%
                \node[rectangle, draw=black!40, top color=white!95!black, bottom color=white!95!black, rounded corners=5pt, inner xsep=5pt, inner ysep=6pt, outer ysep=10pt]{
                \begin{minipage}{0.99\linewidth}#1\end{minipage}};%
            \end{tikzpicture}%
         \end{center}%
}


\tableofcontents


\clearpage

\section{Using Bayes' theorem for statistical inference}

Suppose that an outcome $x$ observed in an experiment is assumed to come from a normal distribution, such that

$f(x;\mu,\sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

where $f(x)$ is the probability density function; $f(x)$ assigns the probability density value to the outcome $x$ conditional on the parameters mean $\mu$ and variance $\sigma^2$ of the normal distribution. The probability density of $x$ conditional on $\mu$ and $\sigma^2$ can be written as,

$p(x|\mu,\sigma^2) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$

The goal of statistical inference is figure out what value(s) of $\mu$ and $\sigma^2$ have generated the observed outcome $x$.

We know the probability density of obtaining $x$ given $\mu$ and $\sigma^2$, can we calculate the probability density of (a range of) values $\mu$ and $\sigma^2$ conditional on the observed outcome $x$?

$p(\mu,\sigma^2|x) = ?$

Using Bayes' theorem,

$p(\mu,\sigma^2|x) = \frac{p(x|\mu,\sigma^2) \cdot p(\mu,\sigma^2)}{\int \int p(x|\mu,\sigma^2) \cdot p(\mu,\sigma^2) \,d \mu \,d\sigma^2}$

\mybox{
More generally, suppose the observed outcome $x$ is assumed to be a value of the random variable $X$ whose probability density function is $f(x;\theta)$; $f(x;\theta)$ assigns a probability density value to $x$ conditional on a parameter $\theta$. The probability density of $x$ given the parameter $\theta$ is given by $p(x|\theta)$.

\vspace{0.2cm}

Our goal is to infer what value(s) of the parameter $\theta$ has generated the given (observed) datapoint $x$.

\begin{equation}
p(\theta|x) = \frac{p(x|\theta) \cdot p(\theta)}{\int p(x|\theta) \cdot p(\theta) \,d \theta}
\end{equation}

The term $p(x|\theta)$ is called the \textbf{likelihood function}, $p(\theta)$ is called the \textbf{prior distribution} of $\theta$, and $p(\theta|x)$ is called the \textbf{posterior distribution} of $\theta$.

\vspace{0.3cm}

Note: When $f(x;\theta)$ is seen as a function of $x$, it is called a probability density function; and when $f(x;\theta)$ is seen as a function of $\theta$, it is called a likelihood function, also denoted by $\mathcal{L}(\theta| x )$.
}


\clearpage

Imagine you collect reading times data in an experiment. Based on your sample of reading times, you try to infer the underlying reality that has generated this observed sample. How do we make this inference using Bayesian modeling?

\section{Unknown reality}

Suppose the reality (the true generative process) that underlies the observed reading times is described as follows.

\begin{enumerate}
\item[] Each individual reading time is (independently) generated by a probability density function $f(x)=\frac{1}{10 \sqrt{2 \pi}} e^{-\frac{(x-300)^2}{200}}$
\end{enumerate}


\section{Observations}

```{r fig.width=6,fig.height=3.5}
y <- rnorm(50,300,10)
hist(y)
```

\section{Assumptions about the generative process}

\begin{enumerate}[label={(\arabic*)}]
\item \textbf{The likelihood assumption:} The observed reading times are normally distributed.

\begin{equation*}
\mathcal{L}(\mu,\sigma|y) = \frac{1}{(\sigma \sqrt{2 \pi})^{^n}} e^{-\frac{1}{2 \sigma^2}\sum_{i=1}^n (y_i-\mu)^2}
\end{equation*}

\noindent where $\sigma = 10$ 


\item \textbf{The prior assumptions:} Based on your prior knowledge or beliefs, suppose you can assume the following about the two parameters of interest.

$\mu \sim Normal(350,50)$

$\sigma = 10$
\end{enumerate}


\section{The likelihood function and the prior distributions}

\subsection{The likelihood}

Each datapoint $y_i$ has a normal distribution:

\begin{equation*}
f(y_i;\mu,\sigma) = \frac{1}{(\sigma \sqrt{2 \pi})^{^n}} e^{-\frac{1}{2 \sigma^2} (y_i-\mu)^2}
\end{equation*}

We can write the above probability density function as a likelihood function. A likelihood function is a function of $\mu$ and $\sigma$ when the data $y_i$ is fixed. The likelihood function is given by

\begin{equation*}
\mathcal{L}(\mu,\sigma|y_i) = \frac{1}{(\sigma \sqrt{2 \pi})^{^n}} e^{-\frac{1}{2 \sigma^2} (y_i-\mu)^2}
\end{equation*}

Observed data: $y_1, y_2, y_3, ..., y_n$

\begin{equation*}
\mathcal{L}(\mu,\sigma|y) = \frac{1}{(\sigma \sqrt{2 \pi})^{^n}} e^{-\frac{1}{2 \sigma^2}\sum_{i=1}^n (y_i-\mu)^2}
\end{equation*}

\noindent where $\sigma = 10$ 

```{r message=FALSE,warning=FALSE}
sigma <- 10
mu <- seq(from=200,to=400,by=0.05)
likelihoods <- data.frame(mu=mu)
likelihoods$lkl <- NA
for(i in 1:length(mu)){
  likelihoods$lkl[i] <- prod(dnorm(y,mu[i],sd=10))
}

ggplot(likelihoods,aes(x=mu,y=lkl))+geom_line(size=1,color="blue")+
  theme_bw()+xlab(expression(mu))+ylab("Likelihood")

ggplot(likelihoods,aes(x=mu,y=lkl))+geom_line(size=1,color="blue")+
  theme_bw()+xlab(expression(mu))+ylab("Likelihood")+
  scale_x_continuous(limits = c(275,325))

ggplot(likelihoods,aes(x=mu,y=lkl))+geom_line(size=1,color="blue")+
  theme_bw()+xlab(expression(mu))+ylab("Likelihood")+
  scale_x_continuous(limits = c(275,325))+
  geom_vline(xintercept = mean(y),color="red",linetype="dashed")
```

\subsection{The priors}

The prior density of $\sigma$ and $\mu$ is given by

\begin{equation*}
p(\sigma) = \Bigg \{ \begin{matrix} 1 & \text{when } \text{  } \sigma=10\\ 0 & \text{when } \text{  } \sigma \neq 10 \end{matrix} \Bigg \}
\end{equation*}

\begin{equation*}
p(\mu) = \frac{1}{\sigma_0 \sqrt{2 \pi}} e^{-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}}
\end{equation*}


\noindent where $\mu_0 = 350$ and $\sigma_0 = 50$.

```{r message=FALSE,warning=FALSE}
likelihoods$prior_density <- NA
for(i in 1:length(mu)){
  likelihoods$prior_density[i] <- dnorm(mu[i],mean=350,sd=10)
}

ggplot(likelihoods,aes(x=mu,y=prior_density))+geom_line(size=1,color="orange")+
  theme_bw()+xlab(expression(mu))+ylab("Prior density")

df.lkl_prior <- melt(likelihoods,id=c("mu"))
df.lkl_prior$variable <- ifelse(df.lkl_prior$variable=="lkl","Likelihood","Prior density")

ggplot(df.lkl_prior,aes(x=mu,y=value,color=variable))+geom_line(size=1)+
  theme_bw()+xlab(expression(mu))+ylab("")+
  scale_x_continuous(limits = c(250,400))+
  facet_wrap(~variable,scales = "free_y",ncol = 1)+
  scale_color_manual(values = c("blue","orange"))
```

\section{Model checking}

The model can be described using the following statements.

\begin{equation*}
y_i \sim Normal(\mu,\sigma)
\end{equation*}

\begin{equation*}
\mu \sim Normal(300,50)
\end{equation*}

\begin{equation*}
\sigma = 10
\end{equation*}

\subsection{Simulating data from the model}

\begin{enumerate}
\item Sample a lot of values for the parameters $\mu$ and $\sigma$ from their priors
\item Generate data from the model for each set of parameter values
\end{enumerate}


```{r}
# Sample from the priors
mu <- rnorm(2000,300,50)
sigma <- rep(10,2000)

# Create a dataframe to store the simulated data

# One way is to simply generate a datapoint 
# corresponding to each value of mu
xsim <- rep(NA,length(mu))
for(i in 1:length(mu)){
  xsim[i]<- rnorm(1,mu[i],sd=sigma[i])
}
hist(xsim)


# But we want to make the simulated data 
# as comparable as we can to the observed data 

# Our observed data y contained 50 observations
# So, we should simulate samples containing 50 observations 
# for each value of mu

# Thus, we need to generate 2000 samples 
# each containing N observations (datapoints)

N <- 50 # I will keep it same as the number of observations in our data y

df.sim <- data.frame(sample = rep(1:2000,each=N),
                     mu=rep(mu,each=N),sigma=rep(sigma,each=N),
                     observation = rep(1:N,2000))

df.sim$ysim <- NA
for(i in 1:length(mu)){
  df.sim[df.sim$sample==i,]$ysim <- rnorm(N,mean=mu[i],sd=sigma[i])
}
```

```{r}
hist(df.sim$ysim)
```

```{r fig.height=6,fig.width=8}
ggplot(subset(df.sim,sample<10),aes(x=ysim))+geom_histogram()+facet_wrap(~sample)
```



\subsection{Prior predictive checks}

```{r fig.height=4,fig.width=6}
ggplot(subset(df.sim,sample<10),aes(x=ysim))+geom_histogram()+
  facet_wrap(~sample)+
  geom_vline(xintercept = mean(y),color="red",linetype="dashed",
             size=1)
```


```{r}
df.sim.summary <- df.sim %>% group_by(sample) %>% 
  summarise(meanRT=mean(ysim),sdRT=sd(ysim))

ggplot(df.sim.summary,aes(x=meanRT))+
  geom_histogram(fill="white",color="black")+
  theme_bw()+xlab("Sample means of the simulated data")

ggplot(df.sim.summary,aes(x=meanRT))+
  geom_histogram(fill="white",color="black")+
  theme_bw()+xlab("Sample means of the simulated data")+
  geom_vline(xintercept = mean(y),color="red",
             linetype="dashed",size=1)
```

\subsection{Prior predictions of the model}

```{r}
# You may need either complete simulated data for prior predictions
# Or just a summary statistic of the simulated samples

# Prior predictions of our model

ggplot(df.sim.summary,aes(x=meanRT))+
  geom_histogram(fill="white",color="black")+
  theme_bw()+
  xlab("Average reading time predicted by the model \n (in milliseconds)")
```

\section{Parameter estimation}

\subsection{Unnormalized posterior density}

Given the likelihood and the prior density functions, we should be able to estimate the unnormalized posterior distribution using Bayes' rule:

\begin{equation*}
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
\end{equation*}

Unnormalized posterior distribution of $\mu$:

\begin{equation*}
p'(\mu|y) = \mathcal{L}(\mu|y) p(\mu)
\end{equation*}

```{r message=FALSE,warning=FALSE,fig.width=6,fig.height=6}
likelihoods$posterior_unnorm <- likelihoods$lkl*likelihoods$prior_density
ggplot(likelihoods,aes(x=mu,y=posterior_unnorm))+geom_line(size=1,color="black")+
  theme_bw()+xlab(expression(mu))+ylab("Posterior density \n Unnormalized")+
  scale_x_continuous(limits = c(250,350))

df.lkl_prior <- melt(likelihoods,id=c("mu"))
df.lkl_prior$variable <- 
  ifelse(df.lkl_prior$variable=="lkl","Likelihood",
         ifelse(df.lkl_prior$variable=="prior_density","Prior density","Unnormalized posterior density"))

ggplot(df.lkl_prior,aes(x=mu,y=value,color=variable))+geom_line(size=1)+
  theme_bw()+xlab(expression(mu))+ylab("")+
  scale_x_continuous(limits = c(250,400))+
  facet_wrap(~variable,scales = "free_y",ncol = 1)+
  scale_color_manual(values = c("blue","orange","black"))+
  theme(legend.position = "none")
```

Given the above estimates of the unnormalized posterior density, can you draw samples from the posterior distribution of $\mu$?

We will need these samples for further modeling: (i) for estimating the 95\% credible interval, (ii) for generating posterior predictions, and  (ii) for model evaluation.

There are two ways in which you can draw samples from the posterior distribution of a parameter:

\begin{enumerate}
\item \label{analytical} Analytically derive the actual, normalized posterior density function $p(\mu|y)$; once you have this function, you can directly sample from this density.

$\mu* \sim p(\mu|y)$

(Although, in practice, you can sample from $p(\mu|y)$ only if it is a well-known standard probability distribution, e.g., a normal distribution.)

\item Evaluate a lot of samples (values) of $\mu$. Accept the samples which have high (unnormalized) posterior density; reject the samples which have low (unnormalized) posterior density. The histogram of the accepted samples should look like a posterior distribution of $\mu$.
\begin{enumerate}
\item \label{manual} Write your own algorithms to draw samples from the posterior.
\item \label{package} Use a pre-defined package (e.g., \textbf{brms/stan}) to draw samples from the posterior. 
\end{enumerate}
\end{enumerate}

These are called \textbf{parameter estimation methods}. We will learn about them in the next module.

\subsection{Analytically-derived posterior distribution}

When 

$y_i \sim Normal(\mu,\sigma)$

and, 

$\mu \sim Normal(\mu_0,\sigma_0)$

and,

$\sigma = \sigma$ 

i.e., when the likelihood is normal and the prior on the mean is normal and the variance is known,

the analytically-derived posterior distribution will be a normal distribution $Normal(\mu',\sigma')$ such that,

\begin{equation*}
\mu|y \sim Normal(\mu',\sigma')
\end{equation*}

\noindent where $\mu|y$ represents a sample from the posterior distribution,

$\mu' = \frac{1}{\Big ( \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2} \Big)} \bigg (\frac{\mu_0}{\sigma_0^2} + \frac{\sum_{i=1}^n y_i}{\sigma^2} \bigg)$

and,

$\sigma' = \frac{1}{\sqrt{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}}$

```{r}
mu_0 <- 350
sigma_0 <- 50
sigma <- 10
n <- length(y)

# Parameters of the posterior distribution
sigma_post <- 1/sqrt((1/sigma_0^2)+(n/sigma^2))
mu_post <- (sigma_post^2)*((mu_0/sigma_0^2)+(sum(y)/sigma^2))

post_samples <- rnorm(10000,mu_post,sigma_post)
hist(post_samples)
```

\clearpage

Does this look similar to the unnormalized posterior density graph?

```{r warning=FALSE,message=FALSE}
df.post_samples <- data.frame(post_samples)
ggplot(df.post_samples,aes(x=post_samples))+
  geom_density(size=1)+theme_bw()+
  scale_x_continuous(limits = c(275,325))

ggplot(likelihoods,aes(x=mu,y=posterior_unnorm))+geom_line(size=1,color="black")+
  theme_bw()+xlab(expression(mu))+ylab("Posterior density \n Unnormalized")+
  scale_x_continuous(limits = c(275,325))
```


\clearpage

\section{Posterior predictions of the model}

\begin{enumerate}
\item Draw a lot of samples from the posterior distribution.
\item Simulate data (50 observations) from the model for each sample.
\end{enumerate}

```{r}
mu_samples <- rnorm(2000,mu_post,sigma_post)
sigma <- rep(10,2000)

N <- 50 # I will keep it same as the number of observations in our data y

df.pred <- data.frame(sample = rep(1:2000,each=N),
                     mu=rep(mu_samples,each=N),sigma=rep(sigma,each=N),
                     observation = rep(1:N,2000))

df.pred$ypred <- NA
for(i in 1:length(mu_samples)){
  df.pred[df.pred$sample==i,]$ypred <- rnorm(N,mean=mu_samples[i],sd=sigma[i])
}
```

```{r}
ggplot(df.pred,aes(x=ypred,group=sample))+
  geom_density(alpha=0.0001)+theme_bw()

obs <- subset(df.pred,sample==1)
obs$ypred <- y

ggplot(df.pred,aes(x=ypred,group=sample))+
  geom_density(alpha=0.0001,color="gray")+
  geom_density(data=obs,aes(x=ypred),color="red",size=1)

ggplot(subset(df.pred,sample<200),aes(x=ypred,group=sample))+
  geom_density(alpha=0.0001,color="gray")+
  geom_density(data=obs,aes(x=ypred),color="red",size=1)
```

\section{Bayesian workflow using brms}

\subsection{Model}

Likelihood:

\begin{equation*}
y_i \sim Normal(\mu, \sigma)
\end{equation*}

Priors:

\begin{equation*}
\mu \sim Normal(350,50)
\end{equation*}

\begin{equation*}
\sigma = 10
\end{equation*}

\subsection{Check prior predictions}

\subsection{Prepare data}

```{r}
dat <- data.frame(trial=1:length(y),y=y)
head(dat)
```


\subsection{Specify the model in the `brm' function}

```{r}
m1 <- brm(y ~ 1, data = dat,
          family = gaussian(),
          prior = c(prior(normal(350,50),class=Intercept),
                    prior(constant(10),class=sigma))
          )
```

```{r}
summary(m1)
```

\subsection{Visualize the posterior samples}

```{r}
plot(m1,pars = c("b_Intercept"))
```

\subsection{Extract the posterior samples}

```{r}
post.samples <- posterior_samples(m1)
head(post.samples)

hist(post.samples$b_Intercept)
```

\subsection{Check posterior predictions}

```{r}
pp_check(m1,ndraws = 100)
```

